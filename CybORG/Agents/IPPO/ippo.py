from CybORG.Agents.IPPO.networks import ActorCritic
from CybORG.Agents.Messages.message_handler import MessageHandler
from CybORG.Agents.IPPO.buffer import ReplayBuffer
from CybORG.Agents.IPPO.rnd_net import RDN_Network
from torch.distributions import Categorical
import torch 
import torch.nn as nn
import numpy as np
import yaml
import os


class PPO:
    def __init__(self, state_dimension, action_dimension, total_episodes, number, messages):
        """
        Args:
            state_dimension (int): The dimension of the state space.
            action_dimension (int): The dimension of the action space.
            total_episodes (int): The total number of episodes for training.
            number (int): The unique agent number.
            messages (bool): Flag to determine whether to use messages.

        Explanation:
            This constructor initializes the PPO agent by setting the hyperparameters, 
            initializing the policy network (actor-critic), setting up the message handler,
            and creating a replay buffer for storing experiences during training.
        """
        self.init_hyperparameters(total_episodes)  # Initialize hyperparameters
        self.memory = ReplayBuffer()  # Initialize replay buffer
        self.init_checkpoint(number)  # Initialize checkpoint paths for saving models
        self.init_check_memory(number)  # Initialize memory for saving statistics
        self.policy = ActorCritic(state_dimension, action_dimension, self.lr, self.eps, self.fc)  # Actor-Critic network
        self.use_messages = messages  # Whether to use messages for communication
        self.agent_number = number  # Agent identifier
        self.message_handler = MessageHandler(message_type=self.message_type, number=self.agent_number)  # Message handler
        self.rdn = RDN_Network(state_dimension)  # RDN Network for exploration rewards


    def get_exploration_reward(self, state):
        """
        Args:
            state (array-like): The current state of the agent.

        Returns:
            float: The intrinsic reward calculated by the RDN Network.

        Explanation:
            This function calculates the intrinsic reward (exploration bonus) for the agent
            based on the current state using the RDN Network.
        """
        return self.rdn.compute_intrinsic_reward(state)


    def get_action(self, state):
        """
        Args:
            state (array-like): The current observation state of the agent.

        Returns:
            action (int): The action chosen to be executed by the agent.
            message (list): A list of messages prepared by the message handler.

        Explanation:
            This function takes the current state as input and computes an action by selecting
            from a probability distribution generated by the policy network (actor). It also
            prepares any relevant messages to send if the flag is set to True.
        """
        normalized_state = (state - np.mean(state)) / (np.std(state) + 1e-8)  # Normalize state
        final_state = torch.FloatTensor(normalized_state.reshape(1,-1))  # Reshape and convert to tensor
        action, logprob, state_value = self.policy.action_selection(final_state)  # Action selection from policy
        # Save state, log probability, action and state value to memory
        self.memory.save_beginning_episode(final_state, logprob, action, state_value)
        
        message = []
        if self.use_messages:
            message = self.message_handler.prepare_message(state, action.item())  # Prepare message if needed
        
        return action.item(), message  # Return selected action and message

    def init_check_memory(self, number):
        """
        Args:
            number (int): The agent number for which statistics are saved.

        Returns:
            None

        Explanation:
            This function initializes lists to save important statistics (like loss values)
            during the training of the agent. It also sets the path where these statistics will be saved.
        """
        self.entropy = []  # List to store entropy values
        self.critic_loss = []  # List to store critic loss values
        self.actor_loss = []  # List to store actor loss values
        self.save_path = f'saved_statistics/ippo/{self.message_type}/data_agent_{number}.csv'  # Path for saving statistics

    
    def load_last_epoch(self):
        """
        Explanation:
            This function loads the most recent saved weights for both actor and critic networks,
            ensuring that the agent continues from the latest training checkpoint.
        """
        print('Loading Last saved Networks......')
        # Load actor and critic network states
        actor_checkpoint = torch.load(self.last_checkpoint_file_actor)
        critic_checkpoint = torch.load(self.last_checkpoint_file_critic)
        self.policy.actor.load_state_dict(actor_checkpoint['network_state_dict'])
        self.policy.critic.load_state_dict(critic_checkpoint['network_state_dict'])
        self.policy.actor_optimizer.load_state_dict(actor_checkpoint['optimizer_state_dict'])
        self.policy.critic_optimizer.load_state_dict(critic_checkpoint['optimizer_state_dict'])


    def load_network(self):
        """
        Explanation:
            This function loads the saved networks (actor and critic) and their corresponding optimizers
            for both networks from the specified checkpoint file paths.
        """
        print('Loading Networks......')
        actor_checkpoint = torch.load(self.checkpoint_file_actor)
        critic_checkpoint = torch.load(self.checkpoint_file_critic)
        self.policy.actor.load_state_dict(actor_checkpoint['network_state_dict'])
        self.policy.critic.load_state_dict(critic_checkpoint['network_state_dict'])
        self.policy.actor_optimizer.load_state_dict(actor_checkpoint['optimizer_state_dict'])
        self.policy.critic_optimizer.load_state_dict(critic_checkpoint['optimizer_state_dict'])


    def init_checkpoint(self, number):
        """
        Args:
            number (int): The agent number.

        Returns:
            None

        Explanation:
            This function initializes the file paths for saving the agent's actor and critic networks,
            both for the current training session and for future use (last saved networks).
        """
        self.checkpoint_file_actor = os.path.join(f'saved_networks/ippo/{self.message_type}', f'actor_ppo_{number}')
        self.checkpoint_file_critic = os.path.join(f'saved_networks/ippo/{self.message_type}', f'critic_ppo_{number}')
        self.last_checkpoint_file_actor = os.path.join(f'last_networks/ippo/{self.message_type}', f'actor_ppo_{number}')
        self.last_checkpoint_file_critic = os.path.join(f'last_networks/ippo/{self.message_type}', f'critic_ppo_{number}')


    def init_hyperparameters(self, episodes):
        """
        Args:
            episodes (int): The total number of episodes for training.

        Returns:
            None

        Explanation:
            This function loads the hyperparameters (learning rate, gamma, clipping factor, etc.)
            from a YAML configuration file and assigns them to instance variables.
        """
        self.max_episodes = episodes  # Set the maximum number of episodes
        config_file_path = os.path.join(os.path.dirname(__file__), 'hyperparameters.yaml')  # Path to YAML file
        with open(config_file_path, 'r') as file:
            params = yaml.safe_load(file)  # Load parameters from YAML file
        # Extract and set hyperparameters
        self.epochs = int(params.get('epochs', 10))
        self.gamma = float(params.get('gamma', 0.99))
        self.clip = float(params.get('clip', 0.1))
        self.lr = float(params.get('lr', 2.5e-4))
        self.min_lr = float(params.get('min_lr', 5e-6))
        self.eps = float(params.get('eps', 1e-5))
        self.gae_lambda = float(params.get('gae_lambda', 0.95))
        self.entropy_coeff = float(params.get('entropy_coeff', 0.01))
        self.value_coefficient = float(params.get('value_coefficient', 0.5))
        self.max_grad_norm = float(params.get('max_grad_norm', 0.5))
        self.minibatch_number = int(params.get('minibatch_number', 1))
        self.fc = int(params.get('fc', 256))
        self.target_kl = float(params.get('target_kl', 0.02))
        self.message_type = params.get('message_type', 'simple')
        self.anneal_type = params.get('lr_anneal', 'linear')


    def anneal_lr(self, steps):
        """
        Args:
            steps (int): The current step or episode number.

        Returns:
            None

        Explanation:
            This function decreases the learning rate over time (annealing), promoting exploitation
            as the agent approaches the maximum number of episodes. It adjusts the learning rate
            based on either linear or exponential annealing strategies.
        """
        frac = (steps - 1) / self.max_episodes  # Calculate fraction of progress in training

        if self.anneal_type == "linear":
            # Linear annealing
            new_lr = self.lr * (1 - frac)
        else:
            # Exponential annealing
            new_lr = self.lr * (self.min_lr / self.lr) ** frac
        
        # Ensure that the learning rate doesn't go below the minimum threshold
        new_lr = max(new_lr, self.min_lr)
    
        # Update the learning rate in the optimizers
        self.policy.actor_optimizer.param_groups[0]["lr"] = new_lr
        self.policy.critic_optimizer.param_groups[0]["lr"] = new_lr


    def anneal_extrinsic_reward(self, steps):
        """
        Args:
            steps (int): The current training step (episode number).

        Returns:
            float: The scaled extrinsic reward.

        Explanation:
            This function gradually decreases the contribution of extrinsic rewards (environment rewards)
            and increases the influence of intrinsic rewards (exploration bonuses) as training progresses.
        """
        frac = (steps - 1) / self.max_episodes  # Calculate progress in training
        new_weight = 0.5 * (1-frac)
        weight = max(new_weight, 0)
        weight = 0.0 # TODO: Comment this if we want NGU rewards
        return weight  # Return linearly decreasing factor for extrinsic reward

    def evaluate(self, observations, actions):
        """
        Args:
            observations (list): A list of states (observations) recorded by the agent.
            actions (list): A list of actions performed by the agent corresponding to each observation.

        Returns:
            state_value (tensor): The value associated with the input observations, obtained from the critic network.
            log_probs (tensor): Log probability of the input actions given the action distribution.
            entropy (tensor): Entropy of the action probability distribution.

        Explanation:
            This function queries both the Actor and Critic networks given the input observations and actions. 
            It computes:
                - The state value by passing the observations through the critic network.
                - The log probabilities of the actions given the policy (actor network).
                - The entropy of the action distribution to measure the randomness of the agent's actions.
        """
        state_value = self.policy.critic(observations).squeeze()  # Get state value from critic
        mean = self.policy.actor(observations)  # Get action probabilities from actor
        dist = Categorical(mean)  # Create categorical distribution based on actor's output
        log_probs = dist.log_prob(actions)  # Get log probability of actions
        entropy = dist.entropy()  # Compute entropy to measure randomness of the distribution
        return state_value, log_probs, entropy  # Return state value, log probabilities, and entropy


    def calculate_gae(self, rewards, values, terminated):
        """
        Args:
            rewards (list): List of rewards (per episode) achieved by the agent.
            values (list): List of state values (per episode) encountered by the agent.
            terminated (list): List of booleans indicating whether the episode ended at each timestep.

        Returns:
            advantage_list (tensor): The computed advantages for each timestep.

        Explanation:
            This function calculates the Generalized Advantage Estimation (GAE) for each timestep. 
            The GAE method helps in reducing the variance of policy gradients while maintaining bias.
            It uses rewards and state values to compute the advantage at each timestep.
            - If the episode is terminated, we don't need to discount future rewards.
            - The advantages are calculated by bootstrapping the future rewards based on the state value function.
        """
        batch_advantage = []  # List to store advantages
        count = 0
        # Loop through each episode (rewards, values, terminated flags)
        for ep_rews, ep_vals, ep_dones in zip(rewards, values, terminated):
            count += 1
            advantages = []  # List to store advantages for current episode
            last_advantage = 0
            # Process the episode backwards (from last timestep)
            for t in reversed(range(len(ep_rews))):
                if t + 1 < len(ep_rews):
                    # TD error calculation for timestep t
                    delta = ep_rews[t] + self.gamma * ep_vals[t + 1] * (1 - ep_dones[t + 1]) - ep_vals[t]
                else:
                    # Last timestep doesn't need future value, just reward - state value
                    delta = ep_rews[t] - ep_vals[t]
                # Apply advantage estimation formula
                advantage = delta + self.gamma * self.gae_lambda * (1 - ep_dones[t]) * last_advantage
                last_advantage = advantage  # Store the advantage for the next timestep
                advantages.insert(0, advantage)  # Insert advantage at the start (to preserve order)
            batch_advantage.extend(advantages)  # Add episode's advantages to the batch
        # Convert the list of advantages to a tensor and return it
        advantage_list = torch.tensor(batch_advantage, dtype=torch.float)
        return advantage_list  # Return the list of advantages


    def save_data(self, entropy_loss, c_loss, a_loss):
        """
        Args:
            entropy_loss (tensor): The computed entropy loss.
            c_loss (tensor): The computed critic loss.
            a_loss (tensor): The computed actor loss.

        Returns:
            None

        Explanation:
            This function stores the computed loss values (entropy loss, critic loss, and actor loss) 
            to be used later for monitoring and debugging purposes.
            - The losses are appended to the corresponding lists (`entropy`, `critic_loss`, `actor_loss`).
        """
        self.entropy.append(entropy_loss.item())  # Save the entropy loss
        self.critic_loss.append(c_loss.item())  # Save the critic loss
        self.actor_loss.append(a_loss.item())  # Save the actor loss


    def learn(self, total_steps):
        """
        Args:
            total_steps (int): The total number of training steps completed so far.
                            This is used to adjust the learning rate during training.

        Returns:
            None

        Explanation:
            This function is the main learning process for the agent. It uses the experience collected
            from previous steps (observations, actions, rewards, etc.) to update the agent's policy.
            The steps followed include:
                1. Get a batch of experiences from memory.
                2. Apply reward scaling and compute advantages using GAE.
                3. Normalize the advantages.
                4. Update the learning rate based on the current training step.
                5. Perform policy updates for the specified number of epochs:
                    - Calculate the policy loss (actor loss) using the PPO clipped objective.
                    - Update the critic network by minimizing the MSE loss between predicted and target values.
                6. Save the loss values and clear memory.
        """
        # Get a batch of experiences
        obs, acts, logprob, rewards, state_val, terminal, intrinsic_rewards = self.memory.get_batch()
        reward_scaler = self.anneal_extrinsic_reward(total_steps)  # Adjust reward scaling
        intrinsic_rewards = [[j * reward_scaler for j in i] for i in intrinsic_rewards]  # Scale intrinsic rewards
        rewards = rewards + intrinsic_rewards  # Add intrinsic rewards to extrinsic rewards
        
        step = acts.size(0)
        index = np.arange(step)  # Array of indices for mini-batch processing
        
        # Initialize loss values
        critic_loss = 0
        actor_loss = 0
        entropy_loss = 0
        
        # Calculate the mini-batch size
        minibatch_size = step // self.minibatch_number
        A_k = self.calculate_gae(rewards, state_val, terminal)  # Calculate advantages
        state_values, _, _ = self.evaluate(obs, acts)  # Get state values for advantage calculation
        
        # Calculate the return (target) values based on advantages and state values
        rtgs = A_k + state_values.detach()
        
        # Normalize the advantages
        A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-8)
        
        # Update the learning rate
        self.anneal_lr(total_steps)
        
        # Perform policy updates for each epoch
        for _ in range(self.epochs):
            np.random.shuffle(index)  # Shuffle the indices for mini-batches
            # Process mini-batches
            for start in range(0, step, minibatch_size):
                end = start + minibatch_size
                idx = index[start:end]
                mini_obs = obs[idx]
                mini_acts = acts[idx]
                mini_log_prob = logprob[idx]
                mini_advantage = A_k[idx]
                mini_rtgs = rtgs[idx]
                
                state_values, curr_log_probs, entropy = self.evaluate(mini_obs, mini_acts)  # Evaluate mini-batch
                
                # Compute entropy loss (to encourage exploration)
                entropy_loss = entropy.mean()
                
                # Calculate the ratio of current log probabilities to old log probabilities
                logrations = curr_log_probs - mini_log_prob
                ratios = torch.exp(logrations)  # Calculate the probability ratio
                approx_kl = ((ratios - 1) - logrations).mean()  # Approximate Kullback-Leibler divergence
                
                # Compute the actor loss using PPO clipped objective
                actor_loss1 = ratios * mini_advantage
                actor_loss2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * mini_advantage
                actor_loss = (-torch.min(actor_loss1, actor_loss2)).mean()
                actor_loss = actor_loss - entropy_loss * self.entropy_coeff  # Adjust by entropy coefficient
                
                # Compute critic loss (Mean Squared Error loss)
                critic_loss = nn.MSELoss()(state_values, mini_rtgs)
                
                # Update the actor network
                self.policy.actor_optimizer.zero_grad()
                actor_loss.backward()
                nn.utils.clip_grad_norm_(self.policy.actor.parameters(), self.max_grad_norm)  # Gradient clipping
                self.policy.actor_optimizer.step()
                
                # Update the critic network
                self.policy.critic_optimizer.zero_grad()
                critic_loss.backward()
                nn.utils.clip_grad_norm_(self.policy.critic.parameters(), self.max_grad_norm)  # Gradient clipping
                self.policy.critic_optimizer.step()

            # Check if the KL divergence is too large, and break early if it is
            if approx_kl > self.target_kl:
                print(f"Breaking Here: {approx_kl}")
                break
        
        # Clear the memory after training
        self.memory.clear_rollout_memory()
        
        # Save the final loss values
        self.save_data(entropy_loss, critic_loss, actor_loss)
        
        # Update the exploration model (RDN)
        self.rdn.anneal_lr()
        self.rdn.update_predictor(obs)
        self.rdn.reset_memory()
